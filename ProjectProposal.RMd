---
title: "Project Proposal for Trends in Programming Language Popularity"
author: "by Indrani Sarkar ,Indranil Maji, Michael Thane, Sharanya Hunasamaranahalli Thotadarya"
date: "20th May,2021"

output:
  rmarkdown::html_document:
    theme: journal
  pdf_document: default
---
[Github Repository Link](https://github.com/mthane/Rscraping)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(kableExtra)
library(dplyr)
library(jsonlite)
library(wordcloud) 
library(tm)
library(vistime)
library(stringr)
```

```{r reddit, echo=FALSE,warning=FALSE,message=FALSE}

       reddit_data <- fromJSON(url(paste("https://www.reddit.com/r/",
                          "coding",
                          "/new/.json?limit=",
                          100,sep="")),flatten = T)$data$children%>%
                      select("data.subreddit",
                             "data.title",
                             #"data.author",
                             #"data.selftext",
                             #"data.name",
                             "data.id",
                             #"data.domain",
                             #"data.url",
                             "data.created",
                             "data.created_utc",
                             "data.upvote_ratio",
                             "data.ups",
                             "data.score",
                             "data.num_comments"
                             )


      
```




# Overview and Motivation


Reddit and Stackoverflow are two community based websites which are used heavily by people all across the world. Reddit is known as the “Front Page of the Internet” and is a popular forum especially among young people where users can post anything and everything.It has a large international community and a lot of programming related content. Reddit is a place for all possible topics of discussion whereas Stack Overflow is centered around programming languages, ideas and discussions surrounding it. Stack overflow is more of a question answer based system. Reddit is mostly a post and comments based system. Both of these websites have an upvote/downvote system and are great resources for coders and programmers around the globe. 

We want to use data from the Reddit forum in order to better understand the popularity of Programming languages among Reddit users.Additionally, we want to compare it to data from the Stack Overflow forum. We want to evaluate what programming languages are being discussed in both forums and compare how their usage or popularity has changed over time. For reaching our aim we want to use Visualization and Machine Learning methods based on text data but also use the quantification that we get from the upvotes and number of comments on both the platforms.

Since both platforms are very popular and most sought after and contain a lot of discussion related to programming, it would be good to study and analyse the trend of how users have been using some of the topmost programming languages. The change of trend would help us in understanding  how the popularity of programming language has changed over time and also we could then predict which programming language would be centre of discussion or most queried of in these two platforms in near future. 


# Project Objective 
The objective of our project is to find a correlation between the different parameters of  questions or posts in these platforms and try to calculate how the popularity of programming language has changed over time. And then further we would like to predict what would be the trends of these programming languages in near future. We could use this information to also relate the kinds of problems or topics which are most related with this programming languages and analyze the kind of topics or solutions most used for certain kinds of problems. We would try to answer the following research questions :

## Related Work

- The growth of R programming language blog by stackoverflow (Link)
- The growth of Python programming language blog by stackoverflow (Link)

The above 2 blogs are a very detailed analysis of how the popularity of programming languages like R and Python grew with time. It has a very intuitive and analytical approach and also gives us data analysis results via usage of meaningful curves which show how questions related to programming languages have been viewed or in which sectors of industry or which domain they have influenced more with a clear comparative analysis. They also give us insights into which packages of a programming language has been used more or even future predictive analysis of how the usage of the programming language will be in the near future.
 
- The PYPL indexing / TIOBE indexing for programming language (Link, Link)
- The PYPL indexing for programming languages is based on google trends of how often problems , questions and queries are made related to certain programming languages and it also has a wonderful dynamic visualisation for easy comparison of the changing trends.
The TIOBE indexing is also another related work which analyses count of web pages involving the programming keywords to decide their popularity and changing trend.


## Research Questions

* Can we decide which topic/language a certain post is about?

The idea is to utilise different features of the data and to decide which features would be best choice for deciding the topic/language a certain post is about. We have several features utilised such as the tags(in case of stackoverflow) , lexical analysis and also to which subreddits they belong to and how often they occur.
* How do number of upvotes, comments and number of posts correlate to popularity?

	As an extension to the previous research question , here we would try to find a correlation between the various features like upvotes , comments and post count to give a more clear picture , about which feature has impacted more or is responsible more in impacting or influencing our results from the analysis.
* How does the popularity of programming languages change over time?

	For answering this our main aim would be to obtain visualisations of the data over the time period of 5 years(2016-2021,present) and then do an analytical as well as visual analysis to conclude how in past the trends have varied and if there is any specific event in the technology universe which has fuelled this.
* Can we predict the popularity of programming languages in the future?

	The idea would be to use forecasting and predictive techniques and see how in near future the popularity or trends in programming language would look like. One important point to consider here would be that the prediction on both the platforms are made using similar or atleast semantically similar features so that the comparison is fair. 
	
* How do the two platforms compare based on programming languages?

This would be our final research question and would be elaborately analysed and explored based on various different factors and analysis. The challenge here would be to keep the features used and the methods use as common as possible so that the comparison is fair and we get a detailed picture of why and how a certain platform is better.

# Datasets

For the Reddit posts the plan is to use an API from Reddit to get data sets for a certain time range and a number of specific Subreddits. The choice of the Subreddits is crucial for the quality and expressiveness of our data and will be based on some prior research on interesting Subreddits regarding programming. From this data we can then get the Subreddit, title, text, upvotes and various metadata.

For Stackoverflow we plan to use the datadumps available on internet archive and then merge them and further preprocess to use it for analysis, visualisation and prediction tasks.


## Data Processing - Reddit Data

### Fetching the data from the API
some text

```{r reddit_scraping, echo=FALSE,warning=FALSE,message=FALSE}
library(tm)
library(data.table)
library(topicmodels)
library(jsonlite)
library(dplyr)
library(ggplot2)
library(stringr)
library(tidyverse)
library(data.table)
library(plotly)
library(tidytext)
library(wordcloud)
library(lubridate)
library(RCurl)

create_redditData <- function(subreddit,
                              nposts=100,
                              from=NA,
                              to=NA,
                              sort_type="created_utc"){
  
  columns <- c("subreddit",
               "title",
               "author",
               "selftext",
               "id",
               "domain",
               "url",
               "created_utc",
               #"upvote_ratio",
               "score",
               "num_comments")
  df = NA
  if(is.na(from) | is.na(to)){
    pushshift_url = paste("https://api.pushshift.io/reddit/search/submission/?",
                          "&size=",nposts,
                          "&subreddit=",subreddit,
                          "&sort_type=",sort_type,
                          sep="")
  }else{
    pushshift_url = paste("https://api.pushshift.io/reddit/search/submission/?",
                          "before=",
                          paste0(to),
                          "&after=",
                          paste0(from),
                          "&size=",nposts,
                          "&subreddit=",subreddit,
                          "&sort_type=",sort_type,
                          sep="")
  }
  tryCatch(
    {
      
      df = fromJSON(URLencode(pushshift_url),flatten = T)$data%>%as.data.frame()
    },
    error=function(cond) {
      message(paste("URL does not seem to exist:", pushshift_url))
      message("Here's the original error message:")
      message(cond)
      return(NA)
    }
  )
  if(length(df)<1){
    return(NA)
  }
  if(!is.na(df)&!is.null(df))
  {
    if(columns %in% colnames(df)& nrow(df)>0){
      
      df%>%select(columns)
    }
  }else{
    print("no data available")
    NA
  }

  
}
```

## Fetching different subreddits
explain function
```{r reddit_scraping_subreddits, echo=TRUE,warning=FALSE,message=FALSE}
#d <- create_redditData("programming")

fetch_subreddits <- function(subreddits,sort_type="created_utc",from=NA,to=NA){
  print(subreddits)
  l = data.frame(subreddit = subreddits,
           sort_type = rep(sort_type,length(subreddits)),
           from = rep(from,length(subreddits)),
           to = rep(to,length(subreddits)))

  rds <- list()
  for (i in 1:length(subreddits)){
    print(l[i,]$subreddit)
    rd <- create_redditData(l[i,]$subreddit,l[i,]$sort_type,l[i,]$from,l[i,]$to)
    Sys.sleep(sample(1:5, 1)/500)
    rds[[i]]<- rd
  }
  bind_rows(rds[!is.na(rds)])
}
```
explain function
```{r reddit_scraping_subreddits2,  echo=TRUE,warning=FALSE,message=FALSE}
fetch_redditData <- function(from,to,subreddits){
  d = difftime(from, to,
               units = c("days"))
  y <- as.POSIXct(from) +lubridate::days(1:as.numeric(d))
  times <- format(round(as.numeric(y), 3), digits = 13)
  to <- times[seq(1,length(times)-1)]
  from <- times[seq(2,length(times))]
  rdfs <- list()
  for (i in 1:length(from)){
    print(round(i/length(from),3))
    rd <- fetch_subreddits(
      subreddits,
      sort_type = "num_comments",
      from= from[i],
      to = to[i]
    )#%>%
      #extract_languages()
    rdfs[[i]] <- rd
  }
  bind_rows(rdfs[!is.na(rdfs)])
  
}

```

### How to get the reddit data?
explain

```{r reddit_scraping_subreddits3, echo=TRUE,warning=FALSE,message=FALSE}
### TESTING 1 day of Reddit Data
time1 = "2020-06-01"
time2 = "2020-06-07"
sreddits <- c(
      "LearnProgramming",
      "AskProgramming",
      "Programming",
      "Coding",
      "datascience",
      "MachineLearning",
      "webdev",
      "Python",
      "javascript",
      "golang",
      "ProgrammerHumor"
    )

#rd <- fetch_redditData(time1,time2,subreddits=sreddits)

# instead of calling the function we load already processed data
rd <- readRDS("reddit_data16-20.Rds")%>%
  select(!c("values","language"))
rd
```
### Remove duplicates

```{r reddit_scraping_subreddits4, echo=FALSE,warning=FALSE,message=FALSE}

```


### Extracting languages

explain

```{r reddit_extract_languages, echo=TRUE,warning=FALSE,message=FALSE}
extract_languages <- function(data){
  
    count_prop <- function(text,word){
      str_count(text,word)/length(strsplit(text," "))
    }
    # adding Ruby, C#, Javascript
    data%>%
        mutate(text_new = paste(selftext,title,sep=" "))%>%
        mutate(text_new =  tolower(text_new))%>%
        mutate(java =count_prop(text_new," java "))%>%
        mutate(python =count_prop(text_new,"python"))%>%
        mutate(r = count_prop(text_new," r "))%>%
        mutate(javascript =count_prop(text_new,"javascript "))%>%
        mutate(c =(count_prop(text_new," c ")))%>%
        mutate(cpp = count_prop(text_new,fixed("c++ ")))%>%
        mutate(csharp =count_prop(text_new," c# "))%>%
        mutate(ruby =count_prop(text_new," ruby "))%>%
        mutate(php =count_prop(text_new," php "))%>%

        mutate(languageprop = java+python+r+javascript+c+csharp+ruby+php)%>%
        pivot_longer(c('java',
                       'python',
                       'r',
                       'javascript',
                       'c',
                       'cpp',
                       'csharp',
                       'ruby',
                       'php'

        ),
        names_to = "language",
        values_to = "values")%>%
        filter(values!=0)%>%
        group_by(id)%>%
        slice_max(values)
       
}

lrd <- extract_languages(rd)
lrd
```





### Explorative Data Analyis and Visualization

Total number of posts per language
1-2 sentences about what we can observe

```{r reddit_counts_bar, echo=TRUE,warning=FALSE,message=FALSE,fig.height=6}
plot_language_counts_bar <- function(redditData){
  redditData%>%
    group_by(language)%>%
    summarise(N =n())%>%
    ggplot(aes(x=reorder(language,-N),y=N))+
       geom_col(fill='#FF5700',alpha=0.7)+
       labs(title="Total number of posts per language",
            y = "Number of posts",
            x = "Programming Language"
            )+
    theme_bw()+
    theme(axis.text.x = element_text(angle = 90))
}
plot_language_counts_bar(lrd)
```



### Average number of comments
1-2 sentences

```{r reddit_avg_comments, echo=TRUE,warning=FALSE,message=FALSE,fig.height=6}
plot_language_comments_bar1 <- function(redditData){ 
redditData%>%
    group_by(language)%>%
    summarise(
      comments = sum(num_comments)/n())%>%
    ggplot(aes(x=reorder(language,-comments),y=comments))+
       geom_col(fill='#FF5700',alpha=0.7)+
       labs(title="Average number of comments per post for all languages",
            y = "Number of comments",
            x = "Programming Language"
            )+
    theme_bw()+
    theme(axis.text.x = element_text(angle = 90))
}
plot_language_comments_bar1(lrd)

```

1-2 sentences

```{r reddit_avg_comments_facet, echo=FALSE,warning=FALSE,message=FALSE,fig.height=6}
plot_language_comments_bar2 <- function(redditData){
  redditData%>%
    group_by(language,subreddit)%>%
    summarise(
      comments = sum(num_comments)/n())%>%
    ggplot(aes(x=reorder(language,-comments),y=comments))+
       geom_col(fill='#FF5700',alpha=0.7)+
       labs(title="Average number of comments per post for all languages and subreddits",
            y = "Number of comments",
            x = "Programming Language"
            )+
       facet_wrap(vars(subreddit))+
    theme_bw()+
    theme(axis.text.x = element_text(angle = 90))
  
  #, scales="free")  
}

plot_language_comments_bar2(lrd)

```


### Text length

### Word clouds for each language

### Trends in Programming Languages

```{r reddit_trends, echo=FALSE,warning=FALSE,message=FALSE,fig.height=6}
# 
# library(stats)
# 
# ma <- function(x, n = 5){stats::filter(x, rep(1 / n, n), sides = 2)}
# lrd%>%
#   mutate(date = as.POSIXct(created_utc, origin="1970-01-01"))%>%
#   
#   group_by(language)%>%
#   #summarise(comments = sum(num_comments))%>%
#   #ungroup()%>%
#   summarise(comments = cut(num_comments,100),date=date)%>%#ma(num_comments,30),date=date)%>%
# 
#   ggplot(aes(x=date,y=comments))+
#     geom_line(color='#FF5700',alpha=0.7)+
#     geom_smooth()+#method="lm")+
#     facet_wrap(vars(language),scales='free')+
#     theme_bw()

```




### Correlation Analysis

```{r reddit_correlation, echo=FALSE,warning=FALSE,message=FALSE}

    count_prop <- function(text,word){
      str_count(text,word)/length(strsplit(text," "))
    }
    # adding Ruby, C#, Javascript
    ld <- rd%>%
        mutate(text_new = paste(selftext,title,sep=" "))%>%
        mutate(text_new =  tolower(text_new))%>%
        mutate(java =count_prop(text_new," java "))%>%
        mutate(python =count_prop(text_new,"python"))%>%
        mutate(r =count_prop(text_new," r "))%>%
        mutate(javascript =count_prop(text_new,"javascript "))%>%
        mutate(c =(count_prop(text_new," c ")))%>%
        mutate(cpp = count_prop(text_new,fixed("c++ ")))%>%
        mutate(csharp =count_prop(text_new," c# "))%>%
      
        mutate(ruby =count_prop(text_new," ruby "))%>%
        mutate(php =count_prop(text_new," php "))


```
One question that arises when looking at the different languages is how they are correlating. Is it for example likely when a certain post is a reddit post is about c, then it might be also about c++?
To approach this question we can first have a look at the correlation matrix.

```{r corplot_reddit1, echo=FALSE,warning=FALSE,message=FALSE,fig.height=10,fig.width=10}

library(ggcorrplot)
cm <- ld %>%
  select(java,python,r,javascript,c,cpp,csharp,ruby,php)
M <-cm[,-1] 
cor(M)
```

For the visualization of those results we can use a color coded correlation matrix.

```{r corplot_reddit2, echo=FALSE,warning=FALSE,message=FALSE,fig.height=10,fig.width=10}

ggcorrplot(cor(M,method = 'pearson') ,hc.order=T,lab = TRUE,type='lower')
#library(corrr)
``` 
Now we can cross out all the non-significant correlations in order to check if those correlations are actually statistically sound.

```{r corplot_reddit3, echo=FALSE,warning=FALSE,message=FALSE,fig.height=10,fig.width=10}
p.mat <- cor_pmat(M)
ggcorrplot(cor(M,method = 'pearson'), p.mat = p.mat ,hc.order=T,type='lower',lab=T)

``` 
Another way to show the correlation is with a graph-based approach. In this network plot it gets more clear how those different programming languages actually correlate.

```{r corplot_reddit4, echo=FALSE,warning=FALSE,message=FALSE,fig.height=10,fig.width=10}
library(corrr)
M%>%correlate(method="pearson")%>%
  network_plot(min_cor=0,curved=T, colors = c("red", "green"))#+
  #theme_gray()

```



### Topic Modeling - Reddit





```{r topic_modeling1, echo=FALSE,warning=FALSE,message=FALSE,fig.height=6}

lda_topic_model <- function(redditData,K){
  text <- redditData$selftext
  df <- data.frame(text = text,doc_id = seq(1,length(text)))%>%na.omit()
  corpus <- VCorpus(DataframeSource(df))
  #tdm <- TermDocumentMatrix(corpus)
  
  english_stopwords <- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")
  # Preprocessing chain
  #message("transform to lower")
  #processedCorpus <- tm_map(corpus, content_transformer(tolower))
  message("remove stopwords")
  processedCorpus <- tm_map(processedCorpus, removeWords, english_stopwords)
  #message("remove punctuation")
  #processedCorpus <- tm_map(processedCorpus, removePunctuation, preserve_intra_word_dashes = TRUE)
  message("remove numbers")
  processedCorpus <- tm_map(processedCorpus, removeNumbers)
  #message("stem document")
  #processedCorpus <- tm_map(processedCorpus, stemDocument, language = "en")
  message("strip whitespace")
  processedCorpus <- tm_map(processedCorpus, stripWhitespace)
  message("create DTM")
  minimumFrequency <- 15
  
  DTM <- DocumentTermMatrix(processedCorpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))
  #rowTotals <- apply(DTM , 2, sum) #Find the sum of words in each Document
  #dtm.new   <- DTM[rowTotals> 0, ]           #remove all docs without words
  #print(dtm.new)
  #LDA(dtm.new,K)
  sel_idx <- slam::row_sums(DTM) > 0
  DTM <- DTM[sel_idx, ]
  
  message("create LDA model")
  m = model = LDA(DTM,K)
  list(dtm = DTM,model = m,corpus = processedCorpus)
}

lda_model <- lda_topic_model(rd,10)
lda_model$model
# ntopics <- FindTopicsNumber(
#   lda_model$dtm,
#   topics = seq(from = 10, to = 50, by = 10),
#   metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
#   method = "Gibbs",
#   control = list(seed = 77),
#   mc.cores = 2L,
#   verbose = TRUE
# )
# ntopics
```
### Optimal number 
```{r topic_modeling2, echo=FALSE,warning=FALSE,message=FALSE,fig.height=6}


```

#### Frequent words


```{r topic_modeling, echo=FALSE,warning=FALSE,message=FALSE,fig.height=6}

ap_topics <- tidy(lda_model$model, matrix = "beta")

plot_frequent_topics <- function(ap_topics,n=20){
  ap_top_terms <- ap_topics %>%
    group_by(topic) %>%
    slice_max(beta, n =n) %>% 
    ungroup() %>%
    arrange(topic, -beta)
  
  ap_top_terms %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free") +
    scale_y_reordered()
  
}
plot_frequent_topics(ap_topics)
```
#### Word clouds

### Forcasting - Reddit



### Shiny application
- Using shiny as a tool for interactive exploration of the data set
  - Sliders for selecting a time range of interest
  - Checkboxes or Dropdown menus to select different programming languages
  - Dropdown to visualize wordcloud on different clusters
  - Further enhancements based on data visualisation
- Bonus: Shiny offers the opportunity of refreshing the data sets on a web page making it possible to get new insights everyday
